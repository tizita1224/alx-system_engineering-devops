Overview of the Problem
There was a significant outage on August 20, 2024, from 3:00 PM to 5:15 PM UTC, affecting our API Gateway for two hours and fifteen minutes. Approximately 95% of our users were left looking at their computers during this time, unable to do anything but refresh their screens incessantly or try another API request. An entire service disruption resulted from this outage, which also caused a sharp decline in our transaction volume and an abundance of customer support tickets. the main reason? Something as annoying as locking your keys inside your car happens when your SSL certificate expires. Our services were rendered completely unresponsive as a result of the SSL handshake errors caused by this expired certificate, which permanently banned all incoming and outgoing API traffic.

Timeline
The issue was first detected at 3:00 PM when our monitoring systems raised the alarm, flagging a sudden and massive spike in API request failures. Just five minutes later, the engineering team received an alert and quickly entered the “What’s going on?” phase of the investigation. Initial thoughts pointed to network issues as the likely culprit—though, as it turned out, this was a dead end. By 3:25 PM, the network team had cleared themselves of any wrongdoing, confirming that everything on their end was functioning normally. At 3:30 PM, the application team shifted focus to the API Gateway and unearthed some suspicious SSL errors in the logs. However, a detour into possible misconfigurations within the API Gateway at 3:45 PM led nowhere. Finally, at 4:00 PM, the security team joined the fray and dropped the bombshell: “Your SSL certificate expired.” The incident was promptly escalated to the DevOps team at 4:10 PM, who leapt into action, renewing and deploying a new SSL certificate. By 4:45 PM, the API Gateway was back online, and normal service was restored. The outage was officially declared over at 5:15 PM.

Root Cause and Resolution
The root cause of this debacle was an expired SSL certificate on the API Gateway. In simple terms, our automated certificate renewal process failed to do its job, leaving the expired certificate in place and bringing our API traffic to a screeching halt due to failed SSL handshakes—essentially a digital high-five gone wrong. Once the expired certificate was identified as the issue, the DevOps team moved swiftly. They generated a new SSL certificate, deployed it to the API Gateway, and restarted the service. With the new certificate in place, SSL handshakes began succeeding again, and our API Gateway was back in action.

Corrective and Preventative Measures
To prevent a repeat of this incident, we’re putting several corrective and preventative measures in place. First, we’re reinforcing our automated SSL certificate renewal process with additional safeguards to ensure that certificates are always renewed on time. We’re also setting up alerts that will notify us well before any certificate is due to expire, with reminders at 30, 15, and 5 days out. Furthermore, we’re integrating SSL certificate expiry checks into our monitoring system, so we can catch any potential issues before they escalate.

Our action items include patching the SSL renewal script to prevent future failures, conducting training for the DevOps team on manual SSL renewal procedures, auditing all SSL certificates across our infrastructure, and developing a detailed incident response playbook specifically for SSL-related issues.
As a lighthearted reminder of the importance of these measures, imagine a cartoonish server holding an expired “ID card” (our SSL certificate) and getting turned away at the gates of the internet, only for a DevOps superhero to swoop in with a shiny new certificate, saving the day. The tagline? “When an SSL certificate takes a vacation, chaos ensues. Thankfully, we’ve got a plan to keep it on the job!”
